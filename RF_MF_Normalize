import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import csv
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import re
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, balanced_accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay
from sklearn.preprocessing import normalize
from ogb.graphproppred import Evaluator

import tensorflow as tf
from keras import Sequential
import keras

from MLPHelper import MLP
from MLPHelper import MyDataset

row_num = -1

for t in [2, 3, 4]:
    for j in [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]:
        for k in [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6]:  

            row_num+= 1 # Since we only need one row at a time

            if(t == 3 or t == 4):
                print("Skipping: nclusters = " + str(t) + ", ncubes = " + str(j) + ", percoverlap = " + str(k))
                continue

            evaluator = Evaluator(name = "ogbg-molhiv")

            dir = "C:/Users/ronan/OneDrive/Documents/GitHub/test/Kepler_Mapper/Outputs/"+ "ParamSearch/nclusters" + str(t) + "_ncubes" + str(j) + "_percoverlap" + str(k) + "/"
            confusion_dir = "C:/Users/ronan/TDAMapperRF/MF_Normalized Confusion/"
            count_dir = "C:/Users/ronan/TDAMapperRF/MF_Normalized Count Of Labels/"

            print("Currently working on: nclusters = " + str(t) + ", ncubes = " + str(j) + ", percoverlap = " + str(k))

            # Load and prepare the dataset
            filename = dir + 'GraphNodeDistributions.csv'  # Update with the actual file path
            data = pd.read_csv(filename)
            X_MF = pd.read_csv('C:/Users/ronan/TDAMapperRF/MorganFingerprint.csv')

            #X = pd.DataFrame(normalize((data.iloc[:, 1:-1]).to_numpy(), axis=1), columns=((data.iloc[:, 1:-1]).columns))

            X_my = data.iloc[:, :-1]  # Exclude first (ID) and last (label) column for features
            y = data.iloc[:, -1]    # Last column as labels

            del X_MF['HIV_active']

            new_df = pd.merge(X_my, X_MF, on='graph number', how="inner")
            
            print(new_df.head())

            #X = pd.DataFrame(normalize((new_df.iloc[:, 1:]).to_numpy(), axis = 1), columns=((new_df.iloc[:, 1:]).columns))  # Why did i normalize this?
            X = pd.DataFrame(new_df.iloc[:, 1:], columns=((new_df.iloc[:, 1:]).columns))  # Why did i normalize this?
            X = X.to_numpy()
            y = y.to_numpy()

            # Split the data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # Use regex since tensors is a string
            pattern = r'\d+'  

            new_y = np.array([int(re.findall(pattern, val)[0]) for val in  y])
            y = new_y

            X_train = torch.from_numpy(X_train).float()
            X_test = torch.from_numpy(X_test).float()

            new = np.array([int(re.findall(pattern, val)[0]) for val in  y_train])
            y_train = torch.from_numpy(new)
 
            y_true = np.array([int(re.findall(pattern, val)[0]) for val in  y_test])
            y_true = torch.tensor(y_true.reshape(-1, 1))

            # New try
            train_dataset = MyDataset(X_train, y_train)
            #test_dataset = MyDataset(X_test, y_true)
            trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=150, shuffle=True, num_workers=0)
            #testloader = torch.utils.data.DataLoader(test_dataset, batch_size=150, shuffle=False, num_workers=0)

            input_size = (X_train.shape)[1]
            hidden_size = 1024
            hidden_size_2 = 512
            output_size = 2

            model = MLP(input_size, hidden_size, hidden_size_2, output_size)
            optimizer = torch.optim.Adam(model.parameters())
            criterion = nn.BCELoss()

            print(model)

            model.train()

            for epoch in range(10):

                current_loss = 0.0
                
                for i, data in enumerate(trainloader, 0):
                    optimizer.zero_grad()

                    x, y = data
                    y = y.type(torch.float32)
                    y = y.unsqueeze(1)

                    outputs = model(x)

                    loss = criterion(outputs, y)

                    loss.backward()
                    optimizer.step()

                    current_loss += loss.item()

                    if i % 40 == 0:
                        print('\tEpoch %d | Batch %d | Loss %6.2f' % (epoch + 1, i, loss.item()))
                        current_loss = 0.0

            model.eval()

            with torch.no_grad():

                y_pred = model(X_test)

                input_dict = {"y_true": y_true, "y_pred": y_pred}

                res = (evaluator.eval(input_dict=input_dict))
                auroc = (res['rocauc'])
                print(f'AUROC: {auroc:.3f}')
        
            '''
            # Evaluate the model
            accuracy = accuracy_score(y_true, y_pred)
            balanced_acc = balanced_accuracy_score(y_true, y_pred)
            print(f'Accuracy: {accuracy:.2f}')
            print(f'Balanced Accuracy: {balanced_acc:.2f}')
            '''
            
            '''
            input_dict = {"y_true": y_true, "y_pred":y_pred}

            res = (evaluator.eval(input_dict=input_dict))
            auroc = (res['rocauc'])
            print(f'AUROC: {auroc:.3f}')

            # AUROC (Note: Only applicable to binary classification tasks)
            try:
                #auroc = roc_auc_score(y_true, clf.predict_proba(X_test)[:, 1])
                auroc2 = roc_auc_score(y_true, y_pred)
                print(f'AUROC2: {auroc2:.2f}')
            except ValueError:
                print("AUROC could not be calculated - this might be due to a multi-class problem or only one class present in y_true.")
            '''
            '''
            # Display Confusion Matrix
            conf_matrix = confusion_matrix(y_test, y_pred)
            disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
            disp.plot(cmap=plt.cm.Blues)
            plt.title('Confusion Matrix')
            plt.savefig(confusion_dir + "nclusters" + str(t) + "_ncubes" + str(j) + "_percoverlap" + str(k) + ".png")
            
            # Plot label distribution
            plt.figure(figsize=(10, 6))
            sns.countplot(x=y.name, data=data)
            plt.title('Counts of Labels')
            plt.xlabel('Label')
            plt.ylabel('Count')
            plt.savefig(count_dir + "nclusters" + str(t) + "_ncubes" + str(j) + "_percoverlap" + str(k) + ".png")
            '''

            # Write to CSV
            csv_columns = ["n_clusters", "n_cubes", "percent_overlap", "AUROC"]

            with open('RF_Metrics_MF_FF_MLP.csv', 'a', newline='') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=csv_columns)
                writer.writerow({'n_clusters': t, 'n_cubes': j, 'percent_overlap': k,"AUROC": auroc})
